<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification(CVPR2020)</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/league.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification(CVPR2020)
                <br>
                <hr>

                <p style="position: relative;left: 20%;margin-top: 10%;">20032202050 左玉晖</p>
            </section>


        </section>


        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                <h2>Person Re-identification</h2>
                <p>简称为ReID，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术。广泛被认为是一个图像检索的子问题。给定一个监控行人图像，检索跨设备下的该行人图像。</p>
            </section>
            <aside class="notes">
                就是我在这个光照视角下检测到这个行人，我换个摄像头换个环境视角还想要能检测到这个行人。
            </aside>
            <!--            <section>-->
            <!--                <h2>The Lorenz Equations</h2>-->
            <!--                \[\begin{aligned}-->
            <!--                \dot{x} &amp; = \sigma(y-x) \\-->
            <!--                \dot{y} &amp; = \rho x - y - xz \\-->
            <!--                \dot{z} &amp; = -\beta z + xy-->
            <!--                \end{aligned} \]-->
            <!--            </section>-->

            <section data-background-transition="zoom">
                <h3>行人重识别可以干什么？</h3>
                <!--                <video data-autoplay src="image/120199711-1-208.mp4"></video>-->
                <img src="image/1608880158932.jpg">
                <aside class="notes">
                    比如去迪士尼，把小孩弄丢了，摄像头多，人工找又慢有难。
                    相似度计算问题
                    智能安防
                    人机交互
                    无人超市
                    覆盖面很广的场所，视频监控，代替人去找。
                    延伸的还有车辆重识别、动物重识别
                </aside>
            </section>


            <!--            <section data-background-transition="zoom">-->
            <!--                <h5 style="float: left;position: absolute;">ReID方法</h5>-->
            <!--                <p class="fragment">基于表征学习的ReID方法</p>-->
            <!--                <p class="fragment">基于度量学习的ReID方法</p>-->
            <!--                <p class="fragment">基于局部特征的ReID方法</p>-->
            <!--                <p class="fragment">基于GAN造图的ReID方法</p>-->
            <!--                <p class="fragment">基于视频序列的ReID方法</p>-->
            <!--            </section>-->

            <section data-background-transition="zoom">
                <h3>时空图卷积，图结构</h3>
                <hr>
                <p class="fragment">图结构中结点的定义是什么？</p>
                <p class="fragment">结点和结点之间的联系是什么？(边)</p>
                <p class="fragment">结点的内在含义是什么？</p>
            </section>
            <aside class="notes">
                我们现在使用的rnncnn等处理的数据，都是在欧式空间下来进行建模的，
                CNN的数据是线性数据，比如音频、图片。GNN的数据主要是关系型数据。
                GNN 在对图形中节点间的依赖关系进行建模方面能力强大，使得图分析相关的研究领域取得了突破性进展。
            </aside>
        </section>

        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                <h2>前人工作及存在问题</h2>
                <hr>
                <p class="fragment">不同摄像头造成行人外观的巨大变化</p>
                <p class="fragment">目标遮挡（Occlusion）导致部分特征丢失</p>
                <p class="fragment">不同的光照导致同一目标的特征差异</p>
                <p class="fragment">不同目标衣服颜色、特征近似导致区分度下降</p>
                <aside class="notes">
                    这也是行人重识别领域面对的普遍问题，对是否能解决解决什么程度有所怀疑。我认为通过这种网络模型变换不可能本质上的提升。
                </aside>
            </section>
            <section data-background-transition="zoom">
                <p>和人脸识别那种，高清的对齐的图像不同</p>
                <hr>
                <img src="image/1608882006023.jpg">
                <aside class="notes">
                    不清晰变清晰，加超分辨率重构
                    遮挡，最难（论文好像除了视频解决没什么解决办法）
                </aside>
            </section>

        </section>

        <section data-background-transition="zoom">
            <h2>基于视频序列的ReID方法</h2>
            <hr>
            <section data-transition="slide-in fade-out">
                <br>
                <p>CNN & RNN</p>
                <img src="image/1608721226762.jpg" width="500px" height="401">
            </section>
            <section data-transition="slide-in fade-out">
                <br>
                <p>temporal pooling & spatial-temporal attenttion</p>
                <p>综合考虑空间位置和特征度深度之间的关系</p>
                <img src="image/1609296760034.jpg" width="850px" height="301">
            </section>
            <section data-transition="slide-in fade-out">
                <p>AMOC(光流法)</p>
                <img src="image/1609563374284.jpg" width="500" height="360">
            </section>

            <aside class="notes">
                CNN & RNN 建模效果有限，或者由于其复杂的结构而难以训练。
                AMOC(光流法) 通常提取光流信息需要用到传统的光流提取算法，但是这些算法计算耗时，并且无法与深度学习网络兼容，并且光流对于遮挡和噪声来说不够健壮。
                temporal pooling & spatial-temporal attenttion 这些方法没有充分考虑人体各部分在不同帧之间的时间关系，效果有限。
            </aside>
        </section>

        <section data-background-transition="zoom">
            <h3>论文对挑战的解决</h3>
            <hr>
            <section>
                <p class="fragment">1、仅使用外观特征不足以区分目标，但它们的身体结构信息是不同的。</p>
                <p class="fragment">办法：利用序列中各部分之间的时空关系可以缓解这些问题。(STGCN)</p>
            </section>
            <section>
                <p class="fragment">
                    2.当边界框不完美，或者存在噪声或遮挡时，基于外观的要素可能效果不好。</p>
                <p class="fragment">办法：显式地利用不同帧之间补丁的时间关系，以缓解遮挡和不对准问题。</p>
            </section>
            <section>
                <p class="fragment">
                    3.基于图像的识别最具挑战性的难点之一是如何区分视觉上相似的身份，而大多数基于图像的方法只能依赖于提取细粒度的外观特征。</p>
                <p class="fragment">
                    办法：在基于视频的Re-ID中，相同身份的结构信息(例如形状信息)将更加完整和精确，因为每个视频具有许多帧，这些帧可能覆盖更多的视图和姿势。</p>
            </section>

            <aside class="notes">
                具体地说，通过连接不同帧的所有块来构建图来建模时间关系，目的是提供不同块之间的互补信息，从而缓解拥塞和错位问题。
                另一方面，我们还考虑了帧内的结构信息，通过为视频中的每一帧构造补丁图来提供互补的外观信息。
                创新性可能不足。
            </aside>
        </section>

        <section data-background-transition="zoom">
            <h3>如何构建图结构</h3>
            <hr>
            <p>将局部特征关系融合</p>
            <img src="image/1609295413363.jpg" width="600px" height="400px">
            <aside class="notes">
                邻接矩阵只能写1，关系紧密程度不能表示，只是个一阶语义。
            </aside>
        </section>


        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                <h2>模型设计</h2>
                <hr>
                <img src="image/1608813802389.jpg">
                <a href="http://10.12.1.150:8888/lab" target="_blank">code</a>
            </section>


            <section data-background-transition="zoom">
                <h4>Temporal branch</h4>
                <img src="image/1608816224035.jpg">
            </section>

            <section data-background-transition="zoom">
                <h4>Structural GCN Module(spatial relations branch)</h4>
                <img src="image/1608816551093.jpg">
            </section>

            <section data-background-transition="zoom">
                <h4>loss function</h4>
                <hr>
                <p>batch hard triplet && softmax cross-entropy</p>
                <p>Triplet loss 需要三份数据(可以从一个batch中选择)</p>
                <img class="r-stretch" src="image/1609302428169.jpg" width="600px" height="400px">
                <aside class="notes">
                    其中Anchor表示当前数据，Positive是跟A相同人的数据，Negative是不同人的数据。
                    当前向量、同一人不同向量、不同人不同向量
                    将一个图像经过特征提取后是一个向量，让这个向量和postive更近更好，让这个向量和negative越远越好。
                </aside>
            </section>

        </section>

        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                <h2>从GCN公式角度看如何建模(空域)</h2>
            </section>
            <section data-background-transition="zoom">
                <h2>GCN机制</h2>
                <hr>
                <p class="fragment">信息传递&信息聚合&信息更新</p>
                <img src="image/1609607176681.jpg">
            </section>
            <section>
                <h3>构建图</h3>
                <hr>
                <p class="fragment">
                    \[\begin{aligned}
                    Z=\sigma(A'FW+b)
                    \end{aligned} \]
                <p>
                <p class="fragment">
                    \[\begin{aligned}
                    Z=f(X,A)=softmax(\hat A'_1 \sigma(\hat A'_0 XW^{(0)}) W^{(1)} )
                    \end{aligned} \]
                <p>

            </section>
<!--            <section>-->
<!--                <h3>信息传递&信息聚合</h3>-->
<!--                <hr>-->
<!--                <p class="fragment">-->
<!--                    G(V,E) 每个patch作为V节点，E是他们之间的关系边-->
<!--                <p>-->
<!--                <p class="fragment">构建边\[\begin{aligned}-->
<!--                    e(x_i,x_j) = \Phi(x_i)^T\Phi(x_j)-->
<!--                    \end{aligned} \]-->
<!--                <p>-->
<!--                <p class="fragment">邻接矩阵归一化\[\begin{aligned}-->
<!--                    A_{(i,j)} = \frac {e^2(x_i,x_j)}{\sum_{j=1}^N e^2(x_i,x_j)}-->
<!--                    \end{aligned} \]-->
<!--                <p>-->
<!--                <p>邻接矩阵A和所有节点的特征向量F进行相乘，获取其他节点特征信息</p>-->
<!--                <p>\[\begin{aligned}-->
<!--                    \hat A=A+\lambda I-->
<!--                    \end{aligned} \]-->
<!--                <p>-->
<!--                <p>-->
<!--                    左乘一个度矩阵右乘一个度矩阵，再归一化\[\begin{aligned}-->
<!--                    A'=\hat D^{-\frac {1}{2}}\hat A\hat D^{-\frac {1}{2}}-->
<!--                    \end{aligned} \]-->
<!--                <p>-->
<!--&lt;!&ndash;                <p class="fragment">&ndash;&gt;-->
<!--&lt;!&ndash;                    \[\begin{aligned}&ndash;&gt;-->
<!--&lt;!&ndash;                    Z=\sigma(A'FW+b)&ndash;&gt;-->
<!--&lt;!&ndash;                    \end{aligned} \]&ndash;&gt;-->
<!--&lt;!&ndash;                <p>&ndash;&gt;-->
<!--&lt;!&ndash;                <p class="fragment">&ndash;&gt;-->
<!--&lt;!&ndash;                    \[\begin{aligned}&ndash;&gt;-->
<!--&lt;!&ndash;                    Z=f(X,A)=softmax(\hat A'_1 \sigma(\hat A'_0 XW^{(0)}) W^{(1)} )&ndash;&gt;-->
<!--&lt;!&ndash;                    \end{aligned} \]&ndash;&gt;-->
<!--&lt;!&ndash;                <p>&ndash;&gt;-->

<!--            </section>-->
        </section>

        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                <h2>实验对比</h2>
            </section>
            <section data-background-transition="zoom">
                <h2>数据集</h2>
                <hr>
                <p> MARS: 有1261个身份id，的17503个tracklet和3,248个distractor序列。</p>
                <p> DukeMTMC-VideoReID: 有1812个身份id，4832个tracklets</p>
                <!--                <p>-->
                <!--                    评估协议：累积匹配特性曲线（CMC）和平均精度（mAP）来评价提出的模型的性能。-->
                <!--                </p>-->
                <aside class="notes">
                    中山大学今年有个新的数据集，史上最大数据集SYSU-30k，
                    「弱监督」行人重识别数据集。
                    还有个问题现在数据集基本全是基于学校来做的，在工厂在商城可能效果不好，
                    最大问题是如何落地，很多论文跨域全完蛋。
                    还有对抗攻击的伪装。(少见)
                </aside>
            </section>

            <section data-background-transition="zoom">
                <h2>评估标准</h2>
                <hr>
                <p>rank1 && rank5</p>
                <p>mAP值</p>
                <img src="image/1609565283548.jpg">
            </section>

            <section data-background-transition="zoom">
                <h2>对比其他模型(怀疑)</h2>
                <hr>
                <img src="image/16072613433181.jpg" width="400px" height="500px">
            </section>
            <section data-background-transition="zoom">
                <h2>三分支搭配对比</h2>
                <hr>
                <img src="image/16072629848124.jpg">
            </section>
            <section data-background-transition="zoom">
                <h2>与全连接对比提现GCN必要</h2>
                <hr>
                <img src="image/16072717851849.jpg">
            </section>

        </section>

        <section data-background-transition="zoom">
            <!--            <section data-background-transition="zoom">-->
            <h2>创新点、贡献</h2>
            <hr>
            <!--            </section>-->
            <section>
                1、利用GCN来模拟人体不同部位在一帧内和帧间的潜在关系，为人们提供更具鉴别力和鲁棒性的信息
                <br>(这个地方可以进一步改进)
            </section>
            <section>
                2、提出了时空GCN框架来联合建模视频层的整体斑块关系和帧级的单个帧的结构信息，该框架可以学习斑块之间的区分和鲁棒的时空关系，从而促进基于视频的Re-ID。
                <br>(有堆积模型的嫌疑)
            </section>

        </section>

        <section data-background-transition="zoom">
            <h2>结论</h2>
            <hr>
            <section>
                <p>1.利用斑块间的时间关系缓解遮挡问题</p>
                <p>利用斑块间的空间关系区分外观相似的歧义样本的有效性。</p>
            </section>

            <section>
                <p>2.提出STGCN模型</p>
                <p>SGCN:空间分支通过建模各帧面片之间的关系来学习人体的结构信息。</p>
                <p>TGCN:时态分支通过对不同帧之间的斑块的时态关系进行建模，可以缓解遮挡问题。</p>
            </section>

            <section>
                <p>
                    采用GNN的特性对提取的行人结构特征进行图建模(空域)，并通过时空图卷积挖掘帧内和帧间的潜在关系(时域)。把他们做联合，增强了区分能力和学习的鲁棒性，对基于视频序列的行人重识别领域中存在遮挡噪声识别性不强的问题，进行了有效解决。</p>
                <aside class="notes">
                    用一句话总结这篇论文。那么什么是行人重识别？什么又是时空图卷积呢
                </aside>
            </section>
        </section>


        <section data-background-transition="zoom">
            <section data-background-transition="zoom">
                <h2>未来方向，存在问题</h2>

            </section>
            <section>
                <p>超分辨率重构</p>
                <p>基于人体骨骼关键点</p>
                <p>各种各样的方法很多</p>
                <p>现在跨域的问题比较难解决，怎么落地？</p>
            </section>

            <section data-background-transition="zoom">
                <p> High-Order Information Matters: Learning Relation and Topology for Occluded Person
                    Re-Identiﬁcation(CVPR2020)</p>
                <hr>
                <p>关键点局部特征提取+图卷积融合关键点特征+图匹配特征融合计算相似度并训练模型</p>
                <!--                <p class="fragment">2.图卷积融合关键点特征</p>-->
                <!--                <p class="fragment">3.基于图匹配的方式来计算相似度并训练模型</p>-->
                <img src="image/1608801804504.jpg">
                <aside class="notes">
                    这里不得不提到，也是CVPR2020旷世研究院的一篇论文。
                    主要解决遮蔽问题。而不是单纯的像他那样把图片画成一块一块的，我觉得这篇论文有点水网络结构。
                    提出了三阶段的模型：1. 关键点局部特征提取；2. 图卷积融合关键点特征 3. 基于图匹配的方式来计算相似度并训练模型。
                    可以套人脸识别。
                    数据集:Occluded- DUKEMTMC (querys 里全是遮蔽现象的数据）
                </aside>
            </section>

            <section>
                <p>两层GCN如何设计可以加深加复杂网络模型？</p>
                <p>解决GCN过平滑问题</p>
            </section>

            <aside class="notes">
                比如，一个图像用resnet提出1024维特征在一万个人里计算余弦相似度或者欧式距离做相似计算。其实没这么简单。
            </aside>
        </section>

        <section data-background-transition="zoom">
            <h2>Thank you Bye</h2>
        </section>

        <!-- <section data-markdown data-separator="---" data-separator-vertical="--"  >
          <script type="text/template">
            ## 关于Re-identification

             <p>简称为ReID，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术。广泛被认为是一个图像检索的子问题。给定一个监控行人图像，检索跨设备下的该行人图像。<p>
            --
            ## 主题1-内容1
            内容1-细节1
            --
            ## 主题1-内容2
            内容1-细节2
            ---
            # 主题2
          </script>
        </section>
-->            </div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
    // More info about config & dependencies:
    // - https://github.com/hakimel/reveal.js#configuration
    // - https://github.com/hakimel/reveal.js#dependencies
    Reveal.initialize({
        math: {
            mathjax: 'plugin/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // 参考 http://docs.mathjax.org/en/latest/config-files.html
        },
        dependencies: [
            {src: 'plugin/markdown/marked.js'},
            {src: 'plugin/markdown/markdown.js'},
            {src: 'plugin/notes/notes.js', async: true},
            {
                src: 'plugin/highlight/highlight.js', async: true, callback: function () {
                    hljs.initHighlightingOnLoad();
                }
            },
            {src: 'plugin/math/math.js', async: true}
        ]


    });
</script>
</body>
</html>
